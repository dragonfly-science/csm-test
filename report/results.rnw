\newpage
\definecolor{black}{rgb}{0,0,0}
\definecolor{darkPurple}{RGB}{93, 22, 123}
\definecolor{Purple}{RGB}{157,45,121}
\definecolor{Pink}{RGB}{235,115,103}
\definecolor{Yellow}{RGB}{252,253,191}
\definecolor{White}{RGB}{255,255,255}
\definecolor{lightGray}{RGB}{180,180,180}


\section{Results}

The assessment of the CHM compared three different pathways to calculate vegetation height.
We first assessed the results from the model developed here against surface models generated by OpenTopography and Vibrant Planet.
We then ground-truthed them against field data (LiDAR verification plots). These quality assessment steps were necessary to determine
if the outputs meet accuracy criteria to ingest them into the machine learning infrastructure. Consecutively, the machine-learning used the
CHM to train two spectral bands (red \& near-infrared) to upscale mean canopy height information to areas within the designated validation
and test area. The prototype splits the Lemon canyon area into separate training, test and validation datasets, representing mean canopy height per Sentinel-2 pixels (10 m) by producing artificial gaps in the output.


\subsection{Accuracy assessment of the CHMs}

The accuracy assessment of the CHMs was conducted using the 2.7-km$^2$ Lemon Canyon test region (\autoref{fig:dsm-assess}). Although
the United States Forest Service (USFS) LiDAR point cloud dates back to 2014, the field plot located within the test region was surveyed in 2015, with trees present classified as \textit{Pinus jeffreyi}\footnote{Access here: \url{https://www.conifers.org/pi/Pinus_jeffreyi.php}}. This pine species is found on arid mountain slopes in southern Oregon, Sierra Nevada, and California and typically reaches heights between 24 and 39~m \citep{kral_pinus_1993}.

%In the following layer comparison we refer to Vibrant Planet products as *.vp, Opentopography as *.topo and Dragonfly as *.fly.

The raw point cloud collected by the USFS for the Tahoe National Forest in 2014\footnote{Tahoe National Forest point cloud: \url{https://portal.opentopography.org/datasetMetadata?otCollectionID=OT.032017.26910.2}} had an average density of 8.93 points m$^{-2}$. The point cloud filtering and labelling processes for the DTM generation from OpenTopography and Dragonfly revealed that the PDAL classification pipeline %replace pipeline
classified 25.1\% more ground points than the CHM.topo,
%vendor model,
increasing the resolution from 3.70 points m$^{-2}$ to 4.63 points m$^{-2}$.

When comparing the DTM outputs directly ($\sim$ 2.6 million pixels),  the outputs from DTM.fly were on average 0.41 ± 0.21 m lower than outputs from DTM.topo. Visual inspection of a hillshade render (terrain shading method) of the DTM.topo and DTM.fly (see \autoref{fig:chm-assess}) confirmed that no above-ground vegetation points were included in the DTM, which would otherwise reveal ``bumps'' at the centre of trees. This visual assessment verified that DTM.fly had not elevated the surface by mis-classifying above-ground vegetation as ground; instead, the model identified a higher level of detail within ground features and incorporated them. For the DSM comparison, there was an average difference of 0.02 ± 1.46~m  between DSM.fly %(\autoref{fig:DSMfly})
and DSM.topo, which incorporated all available points in the surface generation process.

\begin{figure}[H]
  \centering
    \begin{subfigure}{.45\linewidth}
    \captionsetup{justification=centering}
      \caption{Google Earth blend with DSM.fly}\label{fig:DSMflyBlend}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/googleEarthDSMflyBlend.png}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
    \captionsetup{justification=centering}
      \caption{DSM.fly}\label{fig:DSMfly}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/DSMfly.png}
         \end{subfigure}

    \vspace{0.25cm}
    \begin{subfigure}{.45\linewidth}
     \captionsetup{justification=centering}
     \caption{DTM.fly}\label{fig:DTMfly}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/DTMfly.png}
         \end{subfigure}
    \begin{subfigure}{.45\linewidth}
     \captionsetup{justification=centering}
     \caption{DTM.topo}\label{fig:DTMtopo}
           \includegraphics[scale=0.50]{s3/images/lemonCanyon/DTMtopo.png}
         \end{subfigure}
  \caption {Visualisation of surface outputs from the digital surface model (DSM) and the digital terrain model (DTM) developed
  in this project (DSM.fly and DTM.fly) and from the OpenTopography DTM (DTM.topo). Also shown is a blend of DSM.fly with Google Earth.} \label{fig:dsm-assess}
%\label{tab:title}
\end{figure}


\newpage

For the CHM comparison, we used CHM.topo %(\autoref{fig:CHMtopo})
with a maximum canopy height of 36.39~m as a reference, and compared the outputs against outputs from CHM.vp %(\autoref{fig:CHMvp})
and CHM.fly (\autoref{fig:chm-assess}). %(\autoref{fig:CHMfly}).
This comparison showed that outputs from CHM.vp were 3.37 ± 5.44~m taller than outputs from CHM.topo, with a maximum canopy height of 78~m.
At the same time, outputs from CHM.fly were 0.36 ± 2.17~m taller than outputs from CHM.topo,
with a maximum canopy height of 36.56~m.

%CHM.vp is 3.37 ± 5.44 m taller than CHM.topo with a maximum canopy height of 78 m.
%CHM.fly is 0.36 ± 2.17 m taller than CHM.topo with a maximum canopy height of 36.56 m.

The results suggested that CHM.fly calculated higher tree estimates than CHM.topo. This difference was likely due to the enhanced resolution in
the DTM.fly, which was lowering the overall elevation surface by 0.41~m. This effect was further  propagated into the CHM.fly,
 which was on average 0.36~m higher than CHM.topo. Both models, CHM.topo and CHM.fly, labeled trees in Lemon Canyon with a
 maximum height of 36.39~m and 36.56~m, respectively, confirming expected maximum heights documented for \textit{Pinus jeffreyi},
 In contrast, CHM.vp  identified trees to be more than double in height (78~m). Although the LiDAR verification plot located within the test region
  provided accurate tree height measurements for 14 trees,
  we were only able to use dominant trees (a total of 4 trees at 12 to 13~m height) to identify them within the radius of the field plot
  (the method suggested by Vibrant Planet).

  With CHM.fly, which was generated from 2014 data and within a 50~m radius of the field plot
  (established in 2015), we identified several height labels above 11 to 12~m, but not exceeding 13~m. This additional verification step
  confirmed that CHM.fly accurately reflects vertical field measurements. Overall, the results confirmed that
  CHM.fly corresponded with vendor-provided CHM.topo, and that field data can be correlated to the CHM.fly results,
  indicating that the model met the quality criteria to train the prototype.

\begin{figure}[H]
  \centering
    \begin{subfigure}{.45\linewidth}
    \captionsetup{justification=centering}
      \caption{Google Earth imagery}\label{fig:GElc}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/googleEarth.png}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
     \captionsetup{justification=centering}
      \caption{CHM.fly}\label{fig:CHMfly}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/CHMfly.png}
    \end{subfigure}

    \vspace{0.25cm}
    \begin{subfigure}{.45\linewidth}
    \captionsetup{justification=centering}
      \caption{CHM.topo}\label{fig:CHMtopo}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/CHMtopo.png}
          \end{subfigure}
    \begin{subfigure}{.45\linewidth}
      \captionsetup{justification=centering}
      \caption{CHM.vp}\label{fig:CHMvp}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/CHMvp.png}
    \end{subfigure}

    \vspace{0.5cm}
    \begin{tabularx}{0.9\textwidth}{XXXXX}
      \cellcolor{black}\textcolor{White}0 & \cellcolor{darkPurple}\textcolor{White}{10} & \cellcolor{Purple}\textcolor{White}{20} & \cellcolor{Pink}\textcolor{White}{30} & \cellcolor{Yellow}\textcolor{lightGray}{40+ meters}
    \end{tabularx}
  \caption {Visualisation of surface outputs from the Canopy Height Model (CHM) developed
  in this project (CHM.fly), from OpenTopography (CHM.topo), and from Vibrant Planet (CHM.vp).}
  \label{fig:chm-assess}
  %\label{tab:title}
\end{figure}

\newpage

\subsection{Prototype development}

The results for the machine-learning concept revealed that a simple model deployed off a
complex data infrastructure was able to predict canopy height information for areas where these data were unavailable.
The model predicted trees that differed on average 4.052 to 8.5~m in size than trees identified in the validation dataset.
This output was slightly improved for the multi-layer perceptron, which predicted trees that differed on average
 3.947 - 8.43~m in size (\autoref{tab:mean-error}).


\begin{table}[!h]
  \caption{Mean error distribution for the machine-learning concept to predict canopy height.}
 \label{tab:mean-error}
 \begin{center}
 \begin{tabular}{lll}
        \addlinespace
  & Linear regressor & Multi-layer perceptron \\
 Z-score mean absolute error & $0.554 \pm 1.05$ & $0.525 \pm 0.85$ \\
 Un-normalized score & $4.052 \pm 8.5$ & $3.947 \pm 8.43$ \\
\end{tabular}
  \end{center}
\end{table}

%\begin{tabular}{|l|l|l|}
%  \captionsetup{justification=centering}
%  \caption{tabular}{Mean error distribution}
%\label{tab:meanerror}
%\hline & Linear Regressor & Multi-layer perceptron \\
%\hline Z-score mean absolute error & $0.554 \pm 0.492$ & $0.525 \pm 0.332$ \\
%\hline Un-normalized score & $4.052 \pm 4.416$ & $3.947 \pm 4.483$ \\
%\hline
%\end{tabular}
%


%The errors were further analysed across 20 height classes, each representing a canopy height of the region,
%ranging in height from ≤1~m to ≤35~m. The average error from this analysis was
% directly proportional to the predicted height model, where the error bars correspond to prediction residuals (\autoref{fig:poc_train_data}).

% \begin{figure}[h]
% \begin{center}
% \includegraphics[width=0.8\textwidth]{images/MAE-across-binned-tree-heights.png}
% \end{center}
%\caption{Mean Absolute Error of canopy height classes from the machine-learning
% concept across ground-truthed tree height classes. Height in m.}
%    \label{fig:poc_train_data}
%\end{figure}
%Mean Absolute Error

The results from these simple models that were trained across a small test region yielded promising results in that
 the predictions from the  B04 (red) and B08 (near-infrared) bands resembled the ground truth labels (see test images in \autoref{fig:mod-pred}).
%The results from these simple models trained across a small test region yielded promising results as shown in the test images below.
%These images are generated by visualizing the CHM predictions using unseen B4 (red) and B8 (near-infrared) bands.
%It can be seen that the predictions from the B4 and B8 closely resemble the ground truth labels on the far right (\autoref{fig:mod-pred}).


\begin{figure}[h]
\begin{center}
\includegraphics[width=1.0\textwidth]{images/model-predictions.png}
\end{center}
\caption{Model predictions across test dataset vs ground truth labels. The images
were generated by visualising the predictions from the Canopy Height Model
using unseen B04 (red) and B08 (near-infrared) bands.}
\label{fig:mod-pred}
\end{figure}


%\begin{figure}[H]
%\centering
%\includegraphics[scale=0.30]{images/model-predictions.png}
%    \captionsetup{justification=centering}
%    \captionof{figure}{Model predictions across test dataset vs ground truth labels}
%    \label{fig:mod-pred}
%\end{figure}


% When the feature selection was measured using a tree-based machine-learning algorithm,  XGBoost,
% the most influential band for predictions using the simple models was B4 (\autoref{fig:xgboost-tree}).

% \begin{figure}[h]
% \begin{center}
% \includegraphics[scale=0.50]{images/feature-importance.png}
% \end{center}
% \caption{Importance measures ($F$ score) of the tree feature selection by the machine-learning algorithm, XGBoost.}
%    \label{fig:xgboost-tree}
%\end{figure}
