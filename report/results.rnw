\newpage
\definecolor{black}{rgb}{0,0,0}
\definecolor{darkPurple}{RGB}{93, 22, 123}
\definecolor{Purple}{RGB}{157,45,121}
\definecolor{Pink}{RGB}{235,115,103}
\definecolor{Yellow}{RGB}{252,253,191}
\definecolor{White}{RGB}{255,255,255}
\definecolor{lightGray}{RGB}{180,180,180}


\section{Results}

The canopy height model assessment compared three different pathways to calculate vegetation height. We checked our results against surface models generated by OpenTopography and Vibrant Planet and ultimately ground-truthed against field data (LiDAR verification plots). These quality assessments steps were necessary to determine if the outputs meet accuracy criteria to ingest them into the machine learning infrastructure. Consecutively, the machine learning used the CHM to train two spectral bands (red & near-infrared) to upscale mean canopy height information to areas within the designated validation and test area.


\subsection{CHM Accuracy Assessment}

The accuracy assessment for the CHM's has been conducted within the Lemon Canyon, a valley located 50 km North from lake Tahoe (\autoref{fig:DSMflyBlend}, 2.7 km$^2$). While the USFS LiDAR point cloud dates back to 2014, the field plot located within the test region survey has been conducted in 2015 and classifies present trees as \textit{Pinus jeffreyi}\footnote{Access here: \url{https://www.conifers.org/pi/Pinus_jeffreyi.php}}. The species can be found on arid mountain slopes in Southern Oregon, Sierra Nevada and California (Kral., 1993) and typically reaches heights between 24 and 39 m. 

In the following layer comparison we refer to Vibrant Planet products as *.vp, Opentopography as *.topo and Dragonfly as *.fly. 

The raw point cloud collected by the USFS for the Tahoe National Forest in 2014\footnote{Tahoe National Forest point cloud: \url{https://portal.opentopography.org/datasetMetadata?otCollectionID=OT.032017.26910.2}} had an average point density of 8.93 pts m$^{-2}$. The point cloud filtering and labelling processes for the DTM generation from Opentopography and Dragonfly revealed that our PDAL classification pipeline classified 25.1 \% more ground points than the vendor model, raising the resolution from 3.70 pts m$^{-2}$ to 4.63 pts m$^{-2}$. When comparing the DTM outputs directly against each other (~ 2.6 million pixels) we identified that DTM.fly is on average 0.41 ± 0.21 m lower than DTM.topo. A visual inspection of a hillshade render (terrain shading method) of the DTM.topo (\autoref{fig:DTMtopo}) and DTM.fly (\autoref{fig:DTMfly}) confirmed that no aboveground vegetation points have been included in the DTM, which would otherwise reveal 'bumps' at the center of trees. This manual check verifies that DTM.fly has not elevated the surface by misclassifying aboveground vegetation as ground, but rather identifies a higher level of detail within ground features and incorporates them into DTM.fly. For the DSM comparison we calculated that on average there is a 0.02 ± 1.46 m difference between DSM.fly (\autoref{fig:DSMfly}) and DSM.topo which incorporate all available points in the surface generation process. 

\begin{figure}[H]
  \centering
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/googleEarthDSMflyBlend.png}
      \captionsetup{justification=centering}
      \caption{Google Earth blend with DSM.fly}\label{fig:DSMflyBlend}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/DSMfly.png}
      \captionsetup{justification=centering}    
      \caption{DSM Dragonfly}\label{fig:DSMfly}
    \end{subfigure}

    \vspace{0.25cm}
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/DTMfly.png}
      \captionsetup{justification=centering}
      \caption{DTM Dragonfly}\label{fig:DTMfly}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/DTMtopo.png}
      \captionsetup{justification=centering}
      \caption{DTM OpenTopography}\label{fig:DTMtopo}
    \end{subfigure}
  \caption {Surface Output Visualisations} \label{tab:title} 
\end{figure}


\newpage

For our CHM comparison we used CHM.topo (\autoref{fig:CHMtopo}) with maximum canopy height at 36.39 m as a reference and compared them against CHM.vp (\autoref{fig:CHMvp}) and CHM.fly (\autoref{fig:CHMfly}):

CHM.vp is 3.37 ± 5.44 m taller than CHM.topo with a maximum canopy height of 78 m.
CHM.fly is 0.36 ± 2.17 m taller than CHM.topo with a maximum canopy height of 36.56 m.

The results suggested that CHM.fly calculates higher tree estimates than CHM.topo. This is likely due to the enhanced resolution in the DTM.fly which is lowering the overall elevation surface by 0.41 m and this effect is further translated into the CHM.fly which is on average 0.36 m higher than CHM.topo. Both models, CHM.topo and CHM.fly labeled trees in the Lemon Canyon with a maximum height of 36.39 m and 36.56 m respectively confirming expected maximum heights documented for \textit{Pinus jeffreyi}, CHM.vp on the other hand identified trees more than double in height (78 m). While the LiDAR verification plot located within the test region provided accurate tree height measurements for 14 trees, we were only able to use dominant trees (4 total, 12 - 13 m) to identify them within the radius of the field plot (method has been suggested by VP). CHM.fly which has been generated from 2014 data and within a 50 m radius of the field plot (established in 2015) we identified several height labels above 11 - 12 m but not exceeding 13 m. This additionalverification step confirmed that CHM.fly accurately reflects vertical field measurements. Overall the results confirmed that CHM.fly is in line with vendor provided CHM.topo and field data can be approximated to our output, suggesting that the model meets quality criteria to train our prototype. 

\begin{figure}[H]
  \centering
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/googleEarth.png}
      \captionsetup{justification=centering}
      \caption{Google Earth blend with DSM.fly}\label{fig:GElc}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/CHMfly.png} 
      \captionsetup{justification=centering}    
      \caption{CHM Dragonfly}\label{fig:CHMfly}
    \end{subfigure}

    \vspace{0.25cm}
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/CHMtopo.png}
      \captionsetup{justification=centering}
      \caption{CHM OpenTopography}\label{fig:CHMtopo}
    \end{subfigure}
    \begin{subfigure}{.45\linewidth}
      \includegraphics[scale=0.50]{s3/images/lemonCanyon/CHMvp.png}
      \captionsetup{justification=centering}
      \caption{CHM Vibrant Planet}\label{fig:CHMvp}
    \end{subfigure}

    \vspace{0.5cm}
    \begin{tabularx}{0.9\textwidth}{XXXXX}
      \cellcolor{black}\textcolor{White}0 & \cellcolor{darkPurple}\textcolor{White}{10} & \cellcolor{Purple}\textcolor{White}{20} & \cellcolor{Pink}\textcolor{White}{30} & \cellcolor{Yellow}\textcolor{lightGray}{40+ meters}
    \end{tabularx}
  \caption {CHM Output Results} \label{tab:title} 
\end{figure}

\newpage

\subsection{Prototype Development}

The results for the machine-learning concept revealed that a simple model deployed off a complex data infrastructure was able to predict canopy height information for areas where this data was unavailable. The model predicts trees which are on average 4.052 ± 4.416 m taller/smaller than identified in the validation dataset. This output is slightly improved for the multi-layer perceptron, which predicts 3.947 ± 4.483 m taller/smaller trees (\autoref{tab:meanerror}). 


\begin{tabular}{|l|l|l|}
  \captionsetup{justification=centering}
  \captionsetup{justification=centering}
  \captionof{tabular}{Mean error distribution}       
  \label{tab:meanerror}        
\hline & Linear Regressor & Multi-layer perceptron \\
\hline Z-score mean absolute error & $0.554 \pm 0.492$ & $0.525 \pm 0.332$ \\
\hline Un-normalized score & $4.052 \pm 4.416$ & $3.947 \pm 4.483$ \\
\hline
\end{tabular}

When the errors are further analysed across 20 bins each representing a canopy height of the region ranging in height from ≤1m to ≤35m; it can be seen that the average error is directly proportional to the predicted height model (\autoref{fig:poc_train_data}).
\begin{figure}[H]                                                                                                                                                             
\centering                                                                                                                                                                    
\includegraphics[scale=0.50]{images/MAE-across-binned-tree-heights.png}
    \captionsetup{justification=centering}
    \captionof{figure}{Binned M.A.E. across binned canopy heights}       
    \label{fig:poc_train_data}                     
\end{figure}

The results from these simple models trained across a small test region yielded promising results as shown in the test images below. These images are generated by visualizing the CHM predictions using unseen B4 (red) and B8 (near-infrared) bands. 
\begin{figure}[H]                                                                                                                                                                                                                    
\centering       
\includegraphics[scale=0.30]{images/model-predictions.png}                                                                                                       
    \captionsetup{justification=centering}                             
    \captionof{figure}{Model predictions across test dataset vs ground truth labels}
    \label{fig:poc_train_data}                                    
\end{figure}

It can be seen that the predictions from the B4 and B8 closely resemble the ground truth labels on the far right.

When feature selection is measured using XGBoost, a tree-based machine learning algorithm, we find that the most influential band for prediction using the simple models above is B4 (\autoref{fig:poc_train_data}).

\begin{figure}[H]                                                                                                                                                                                                            
\centering       
\includegraphics[scale=0.50]{images/feature-importance.png}
    \captionsetup{justification=centering}                                                                                                                                    
    \captionof{figure}{XGBoost tree feature importance measures}
    \label{fig:poc_train_data}                                                      
\end{figure}
